{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb9e832-dbe2-45d2-973a-1dce67d5e8dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e09e7a2b-e81b-4661-a10e-165aa7ac951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai_client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dc9f8e7-782c-4ba0-a3f0-822e93688137",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_client = OpenAI( base_url = \"https://api.groq.com/openai/v1\", api_key = os.environ.get(\"GROQ_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bd54f50-e4f9-414f-b902-6a359d875d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = groq_client.responses.create(\n",
    "  model= \"openai/gpt-oss-20b\",\n",
    "  input= \"Write a short bedtime story about a unicorn.\",\n",
    ");\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db42e3b-5584-4319-a5c4-2ac1d1c3b1de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_01kaxjgk76evx9wr66f0e3qbfz', created_at=1764076571.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='openai/gpt-oss-20b', object='response', output=[ResponseReasoningItem(id='resp_01kaxjgk76evxr50mrnf2rpa00', summary=[], type='reasoning', content=[Content(text='User wants: \"Write a short bedtime story about a unicorn.\" We need to produce a short bedtime story about a unicorn. Probably a gentle, soothing narrative, with a unicorn, suitable for bedtime. The user didn\\'t specify any other constraints. So produce a short bedtime story, likely 4-8 paragraphs, gentle tone. No disallowed content. We comply.', type='reasoning_text')], encrypted_content=None, status='completed'), ResponseOutputMessage(id='msg_01kaxjgk76evy9a99z1q06xvaj', content=[ResponseOutputText(annotations=[], text='**The Moonlit Meadow and the Gentle Unicorn**\\n\\nWhen the stars began to twinkle and the moon climbed high above the hills, Lily the little fox curled up in a nest of soft moss. The night air was cool, and a gentle hush settled over the forest. Lily was nervous, for it was her first night sleeping alone in the forest, and her paws felt a little too big for the small bed she’d made.\\n\\nJust then, a faint, shimmering glow drifted through the trees. Lily’s eyes widened. From the silver light stepped a unicorn—softly glowing, with a coat that looked like moonlight and hooves that whispered on the grass.\\n\\n\"Hello, Lily,\" the unicorn said, her voice like a lullaby. \"I am Luna, and I’ve been watching over this meadow for many moons. I see you’re ready for a story.\"\\n\\nLily’s ears twitched with excitement. She settled into the moss, and Luna lowered herself beside her, her mane brushing against Lily’s tail. Luna began to hum a gentle tune, and the meadow seemed to breathe with the music. She sang about the meadow’s hidden treasures: the sparkling dew that turned into tiny crystals in the morning, the whispering willow trees that told stories of old, and the friendly fireflies that danced at dusk.\\n\\nAfter the song, Luna told Lily about the secret path that led to the crystal pond. The pond reflected the stars so perfectly that it looked like a piece of night sky, and the water was said to be as cool as a sigh. Luna explained that every creature that drank from the pond felt a warm, gentle peace settle in their heart, easing them into dreams as soft as feather.\\n\\nLily listened, her eyes growing heavier. The moonlight grew brighter, and Luna’s glow shimmered like a second star. “Close your eyes, little friend,” Luna whispered. “Let the meadow lull you, and let the dreams take you to places of wonder.”\\n\\nLily curled into the moss, feeling Luna’s gentle warmth. Her breathing slowed, and her heart beat like a soft drum. As the meadow fell silent, the unicorn’s breath ruffled the leaves in a slow rhythm that matched Lily’s own sighs. The world around them faded into a hush of silver light and quiet dreams.\\n\\nWhen Lily finally fell asleep, the unicorn’s glow faded to a gentle twinkle, and she drifted into a sleep filled with rainbow dreams, friendly forest friends, and the soft glow of the moonlit meadow. In the morning, when Lily awoke, she found a single, tiny crystal of dew on her paw, a reminder that Luna had indeed walked the path and that every night can hold a gentle, glowing friend.', type='output_text', logprobs=None)], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort='medium', generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity=None), top_logprobs=None, truncation='disabled', usage=ResponseUsage(input_tokens=80, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=635, output_tokens_details=OutputTokensDetails(reasoning_tokens=74), total_tokens=715), user=None, groq=None, store=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b03f2c19-0dd2-41e0-86a4-85772834e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36bdca61-5d84-49f2-b6b1-ce1e5b4aedbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'It depends on your background and previous experience with modules. It is expected to require about 5 - 15 hours per week. [source1] [source2]\\nYou can also calculate it yourself using this data and then update this answer.',\n",
       " 'section': 'General course-related questions',\n",
       " 'question': 'Course - \\u200b\\u200bHow many hours per week am I expected to spend on this  course?',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5962793-0087-4f71-b013-885e0824098e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from minsearch import AppendableIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae5b63a-0833-405a-b981-0aa5a843a386",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x7d6c1e1952e0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9d849ea-ad75-4814-b64f-e3471d8fa23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'I just discovered the course. Can i join it now' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8410796f-649c-4bcf-9d20-64da2ad4f5aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - When will the course start?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\\nThe course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\\nShould you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Is it possible to use tool “X” instead of the one tool you use in the course?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\nHaving this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\nThis is also a great resource: https://dangitgit.com/',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How do I use Git / GitHub for this course?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Star the repo! Share it with friends if you find it useful ❣️\\nCreate a PR if you see you can improve the text or the structure of the repository.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'How can we contribute to the course?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Yes, the slack channel remains open and you can ask questions there. But always sDocker containers exit code w search the channel first and second, check the FAQ (this document), most likely all your questions are already answered here.\\nYou can also tag the bot @ZoomcampQABot to help you conduct the search, but don’t rely on its answers 100%, it is pretty good though.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I get support if I take the course in the self-paced mode?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You can start by installing and setting up all the dependencies and requirements:\\nGoogle cloud account\\nGoogle Cloud SDK\\nPython 3 (installed with Anaconda)\\nTerraform\\nGit\\nLook over the prerequisites and syllabus to see if you are comfortable with these subjects.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - What can I do before the course starts?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search(query=question,filter_dict={'course': 'data-engineering-zoomcamp'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c357ca43-fe58-4aa6-95d6-e56777022fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search(query: str):\n",
    "    \"\"\"\n",
    "    Search the FAQ database for entries matching the given query.\n",
    "\n",
    "    Args:\n",
    "        query (str): Search query text to look up in the course FAQ.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.\n",
    "    \"\"\"\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20606dd1-0090-4bba-87cb-181696d7a941",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"Yes, even if you don't register, you're still eligible to submit the homeworks.\\nBe aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I still join the course after the start date?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.',\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - Can I follow the course after it finishes?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"The purpose of this document is to capture frequently asked technical questions\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\\nSubscribe to course public Google Calendar (it works from Desktop only).\\nRegister before the course starts using this link.\\nJoin the course Telegram channel with announcements.\\nDon’t forget to register in DataTalks.Club's Slack and join the channel.\",\n",
       "  'section': 'General course-related questions',\n",
       "  'question': 'Course - When will the course start?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search(query='I just discovered the course. Can i join it now')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3dcd266-bb1d-4067-9289-8f8e83d619cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4033f12c-d112-4fda-aa75-f03aa737738e",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "response = openai_client.responses.create(\n",
    "  model= 'gpt-4o-mini',\n",
    "  input= chat_messages,\n",
    "  tools=[search_tool]\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "934d031c-6299-4276-bfd0-91ac1943d4ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_0b7a3c517100453600692581e39b7c819399046df4584113fd', created_at=1764065763.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"query\":\"Can I join the course now?\"}', call_id='call_J00U3J6GqKKQZuZ4RYfxPj30', name='search', type='function_call', id='fc_0b7a3c517100453600692581e4c13081939a12b4f987ea3c13', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='search', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query text to look up in the course FAQ.'}}, 'required': ['query'], 'additionalProperties': False}, strict=True, type='function', description='Search the FAQ database')], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=170, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=20, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=190), user=None, billing={'payer': 'developer'}, store=True)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "028deba2-f0ba-4301-968f-426f6862f53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "call = response.output[0]\n",
    "\n",
    "results = search(\"join course\")\n",
    "result_json = json.dumps(results, indent=2)\n",
    "\n",
    "chat_messages.append({\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.id,\n",
    "    \"output\": result_json,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1f49f8a4-4a45-45de-b506-f67bfc605ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': 'I just discovered the course. Can i join it now'},\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'resp_01kax4qcvbfr1v4hmmebkp6cfq',\n",
       "  'output': '[\\n  {\\n    \"text\": \"Yes, even if you don\\'t register, you\\'re still eligible to submit the homeworks.\\\\nBe aware, however, that there will be deadlines for turning in the final projects. So don\\'t leave everything for the last minute.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - Can I still join the course after the start date?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"The purpose of this document is to capture frequently asked technical questions\\\\nThe exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  \\\\u201cOffice Hours\\'\\' live.1\\\\nSubscribe to course public Google Calendar (it works from Desktop only).\\\\nRegister before the course starts using this link.\\\\nJoin the course Telegram channel with announcements.\\\\nDon\\\\u2019t forget to register in DataTalks.Club\\'s Slack and join the channel.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - When will the course start?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\\\\nYou can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Course - Can I follow the course after it finishes?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"No, you can only get a certificate if you finish the course with a \\\\u201clive\\\\u201d cohort. We don\\'t award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"Certificate - Can I follow the course in a self-paced mode and get a certificate?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  },\\n  {\\n    \"text\": \"After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\\\\nHaving this local repository on your computer will make it easy for you to access the instructors\\\\u2019 code and make pull requests (if you want to add your own notes or make changes to the course content).\\\\nYou will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\\\\nRemember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\\\\nThis is also a great resource: https://dangitgit.com/\",\\n    \"section\": \"General course-related questions\",\\n    \"question\": \"How do I use Git / GitHub for this course?\",\\n    \"course\": \"data-engineering-zoomcamp\"\\n  }\\n]'}]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a70af338-f185-4b3c-9739-4fba3af21f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def make_call(call):\n",
    "    args = json.loads(call.arguments)\n",
    "    f_name = call.name\n",
    "    f = globals()[f_name]\n",
    "    result = f(**args)\n",
    "    result_json = json.dumps(result, indent=2)\n",
    "    return {\n",
    "        \"type\": \"function_call_output\",\n",
    "        \"call_id\": call.call_id,\n",
    "        \"output\": result_json,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c3368c2-feda-4c47-ae32-79a3f4b597a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function call: search {\"query\":\"Can I join the course now?\"}\n",
      "results: [\n",
      "  {\n",
      "    \"text\": \"Yes, even if you don't register ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "\n",
    "    if entry.type == 'message':\n",
    "        print('assistant')\n",
    "        print(entry.content[0].text)\n",
    "\n",
    "    if entry.type == 'function_call':\n",
    "        print('function call:', entry.name, entry.arguments)\n",
    "        result = make_call(entry)\n",
    "        print('results:', result['output'][:50], '...')\n",
    "        chat_messages.append(result)\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e71203fa-77ce-41e3-a7d0-07657494e031",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = openai_client.responses.create(\n",
    "    model='gpt-4o-mini',\n",
    "    input=chat_messages,\n",
    "    tools=[search_tool]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8f0f6da7-c05b-4e2e-bc23-094459c72e34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Response(id='resp_02c2a5e270f8fc4e0069258289ee348198970bcf105a4ece3a', created_at=1764065929.0, error=None, incomplete_details=None, instructions=None, metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseFunctionToolCall(arguments='{\"query\":\"Can I join the course now\"}', call_id='call_361e6pD3r3ZHQIubAs4I1eLK', name='search', type='function_call', id='fc_02c2a5e270f8fc4e006925828ba3488198a92963f59619841a', status='completed')], parallel_tool_calls=True, temperature=1.0, tool_choice='auto', tools=[FunctionTool(name='search', parameters={'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query text to look up in the course FAQ.'}}, 'required': ['query'], 'additionalProperties': False}, strict=True, type='function', description='Search the FAQ database')], top_p=1.0, background=False, conversation=None, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, prompt_cache_retention=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=170, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=19, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=189), user=None, billing={'payer': 'developer'}, store=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "04b0ccc5-1a14-46df-9964-553e25ac7caa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " I just discovered the course. Can i join it now\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tools' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     26\u001b[39m chat_messages.append(message)\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m: \u001b[38;5;66;03m# request-response loop - query API till get a message\u001b[39;00m\n\u001b[32m     29\u001b[39m     response = client.responses.create(\n\u001b[32m     30\u001b[39m         model=\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     31\u001b[39m         \u001b[38;5;28minput\u001b[39m=chat_messages,\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m         tools=\u001b[43mtools\u001b[49m\n\u001b[32m     33\u001b[39m     )\n\u001b[32m     35\u001b[39m     has_tool_calls = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m response.output:\n",
      "\u001b[31mNameError\u001b[39m: name 'tools' is not defined"
     ]
    }
   ],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "If you want to look up the answer, explain why before making the call. Use as many \n",
    "keywords from the user question as possible when making first requests.\n",
    "\n",
    "Make multiple searches. Try to expand your search by using new keywords based on the results you\n",
    "get from the search.\n",
    "\n",
    "At the end, make a clarifying question based on what you presented and ask if there are \n",
    "other areas that the user wants to explore.\n",
    "\"\"\".strip()\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt}\n",
    "]\n",
    "\n",
    "client = openai_client\n",
    "tools = [\n",
    "\n",
    "while True: # main Q&A loop\n",
    "    question = input() # How do I do my best for module 1?\n",
    "    if question == 'stop':\n",
    "        break\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True: # request-response loop - query API till get a message\n",
    "        response = client.responses.create(\n",
    "            model='gpt-4o-mini',\n",
    "            input=chat_messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        has_tool_calls = False\n",
    "        \n",
    "        for entry in response.output:\n",
    "            chat_messages.append(entry)\n",
    "        \n",
    "            if entry.type == 'function_call':      \n",
    "                print('function_call:', entry)\n",
    "                print()\n",
    "                result = make_call(entry)\n",
    "                chat_messages.append(result)\n",
    "                has_tool_calls = True\n",
    "\n",
    "            elif entry.type == 'message':\n",
    "                print(entry.content[0].text)\n",
    "                print()\n",
    "\n",
    "        if not has_tool_calls:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7e51acb-84dc-4781-bf32-f6f165074f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.llm import OpenAIClient\n",
    "from toyaikit.tools import Tools\n",
    "from toyaikit.chat import IPythonChatInterface\n",
    "from toyaikit.chat.runners import OpenAIResponsesRunner\n",
    "from toyaikit.chat.runners import DisplayingRunnerCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3c86fec9-48c9-4969-8eb8-b54c1d47e5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools = Tools()\n",
    "agent_tools.add_tool(search)\n",
    "\n",
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "40206c42-31e9-4518-811d-f8b6def9b40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: HOw dO i install kafka?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To help you with the installation of Kafka, I'll first look for the basic steps and requirements for setting up Kafka. This might include prerequisites like Java installation, downloading Kafka, and configuration.</p>\n",
       "<p>Let me search for that information now.</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"install Kafka prerequisites steps\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"install Kafka prerequisites steps\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - What are the prerequisites for this course?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"kafka.errors.NoBrokersAvailable: NoBrokersAvailable\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Kafka- python videos have low audio and hard to follow up\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \\\"main\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to install Kafka steps instructio...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"how to install Kafka steps instructions configuration Java\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"In my set up, all of the dependencies listed in gradle.build were not installed in &lt;project_name&gt;-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \\\"java-kafka-rides\\\"\\narchiveClassifier = ''\\n}\\nAnd then in the command line ran \\u2018gradle shadowjar\\u2019, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: &lt;project_name&gt;-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \\\"main\\\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"In the project directory, run:\\njava -cp build/libs/&lt;jar_name&gt;-1.0-SNAPSHOT.jar:out src/main/java/org/example/JsonProducer.java\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: How to run producer/consumer/kstreams/etc in terminal\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Make sure your java version is 11 or 8.\\nCheck your version by:\\njava --version\\nCheck all your versions by:\\n/usr/libexec/java_home -V\\nIf you already have got java 11 but just not selected as default, select the specific version by:\\nexport JAVA_HOME=$(/usr/libexec/java_home -v 11.0.22)\\n(or other version of 11)\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Python Kafka: ./spark-submit.sh streaming.py Error: py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn\\u2019t see it at first and had to do some fixes.\\nSolution:\\n(Source)\\nVS Code\\n\\u2192 Explorer (first icon on the left navigation bar)\\n\\u2192 JAVA PROJECTS (bottom collapsable)\\n\\u2192  icon next in the rightmost position to JAVA PROJECTS\\n\\u2192  clean Workspace\\n\\u2192 Confirm by clicking Reload and Delete\\nNow you will be able to see the triangle icon next to each test like what you normally see in python tests.\\nE.g.:\\nYou can also add classes and packages in this window instead of creating files in the project directory\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"Java Kafka: Tests are not picked up in VSCode\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To install Kafka, you can follow these general steps:</p>\n",
       "<ol>\n",
       "<li><p><strong>Install Java</strong>: Kafka requires Java to run. Ensure you have either Java 8 or 11 installed on your machine. You can check the version with:</p>\n",
       "<pre><code class=\"language-bash\">java --version\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Download Kafka</strong>: Go to the <a href=\"https://kafka.apache.org/downloads\">Apache Kafka website</a> and download the latest version.</p>\n",
       "</li>\n",
       "<li><p><strong>Extract the Kafka Files</strong>: After downloading, extract the tar or zip file. You can do this using the command:</p>\n",
       "<pre><code class=\"language-bash\">tar -xzf kafka_2.13-&lt;version&gt;.tgz\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start Zookeeper</strong>: Kafka requires Zookeeper to manage its distributed systems. You can start Zookeeper with the following command:</p>\n",
       "<pre><code class=\"language-bash\">bin/zookeeper-server-start.sh config/zookeeper.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Start Kafka Server</strong>: In a new terminal, run the Kafka server:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-server-start.sh config/server.properties\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Create a Topic</strong> (optional but usually necessary): You can create a topic with:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-topics.sh --create --topic &lt;topic_name&gt; --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Produce and Consume Messages</strong>: You can start producing messages with:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-console-producer.sh --topic &lt;topic_name&gt; --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "<p>And consume them with:</p>\n",
       "<pre><code class=\"language-bash\">bin/kafka-console-consumer.sh --topic &lt;topic_name&gt; --from-beginning --bootstrap-server localhost:9092\n",
       "</code></pre>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Make sure to look up any specific configurations you may need depending on your environment and use case.</p>\n",
       "<p>Does this help with your installation? Are there any specific configurations or topics you would like to explore further?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: STOP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "result = runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5655345a-bd3d-4615-8b1b-551c7b7db7af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CostInfo(input_cost=Decimal('0.00048795'), output_cost=Decimal('0.000297'), total_cost=Decimal('0.00078495'))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "15970f32-9291-41de-973a-4888c585d3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydantic-ai\n",
      "  Downloading pydantic_ai-1.22.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting pydantic-ai-slim==1.22.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading pydantic_ai_slim-1.22.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: genai-prices>=0.0.40 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.0.45)\n",
      "Collecting griffe>=1.3.2 (from pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading griffe-1.15.0-py3-none-any.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.28.1)\n",
      "Collecting opentelemetry-api>=1.28.0 (from pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading opentelemetry_api-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting pydantic-graph==1.22.0 (from pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading pydantic_graph-1.22.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: pydantic>=2.10 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.12.4)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.4.2)\n",
      "Collecting ag-ui-protocol>=0.1.8 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading ag_ui_protocol-0.1.10-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting starlette>=0.45.3 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting anthropic>=0.70.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading anthropic-0.75.0-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting boto3>=1.40.14 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading boto3-1.41.3-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting argcomplete>=3.5.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading argcomplete-3.6.3-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: prompt-toolkit>=3 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (3.0.52)\n",
      "Collecting pyperclip>=1.9.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading pyperclip-1.11.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting rich>=13 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading rich-14.2.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cohere>=5.18.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading cohere-5.20.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pydantic-evals==1.22.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading pydantic_evals-1.22.0-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting fastmcp>=2.12.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading fastmcp-2.13.1-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting google-genai>=1.51.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading google_genai-1.52.0-py3-none-any.whl.metadata (46 kB)\n",
      "Collecting groq>=0.25.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading groq-0.36.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting huggingface-hub>=0.33.5 (from huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading huggingface_hub-1.1.5-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting logfire>=3.14.1 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading logfire-4.15.1-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting mcp>=1.18.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading mcp-1.22.0-py3-none-any.whl.metadata (85 kB)\n",
      "Collecting mistralai>=1.9.10 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading mistralai-1.9.11-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: openai>=1.107.2 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.8.1)\n",
      "Collecting tenacity>=8.2.3 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting temporalio==1.19.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading temporalio-1.19.0-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (92 kB)\n",
      "Collecting google-auth>=2.36.0 (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading google_auth-2.43.0-py2.py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.32.5)\n",
      "Requirement already satisfied: anyio>=0 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-evals==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (4.11.0)\n",
      "Collecting logfire-api>=3.14.1 (from pydantic-evals==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading logfire_api-4.15.1-py3-none-any.whl.metadata (972 bytes)\n",
      "Requirement already satisfied: pyyaml>=6.0.2 in /home/codespace/.local/lib/python3.12/site-packages (from pydantic-evals==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (6.0.3)\n",
      "Collecting nexus-rpc==1.1.0 (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading nexus_rpc-1.1.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting protobuf<7.0.0,>=3.20 (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
      "Collecting types-protobuf>=3.20 (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading types_protobuf-6.32.1.20251105-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.2.0 in /home/codespace/.local/lib/python3.12/site-packages (from temporalio==1.19.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (4.15.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from pydantic>=2.10->pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.41.5)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from anthropic>=0.70.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (1.9.0)\n",
      "Collecting docstring-parser<1,>=0.15 (from anthropic>=0.70.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading docstring_parser-0.17.0-py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from anthropic>=0.70.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.12.0)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.12/site-packages (from anthropic>=0.70.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (1.3.1)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.12/site-packages (from anyio>=0->pydantic-evals==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (3.11)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2025.10.5)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.12/site-packages (from httpx>=0.27->pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /home/codespace/.local/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.16.0)\n",
      "Collecting botocore<1.42.0,>=1.41.3 (from boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading botocore-1.41.3-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.16.0,>=0.15.0 (from boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading s3transfer-0.15.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/codespace/.local/lib/python3.12/site-packages (from botocore<1.42.0,>=1.41.3->boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /home/codespace/.local/lib/python3.12/site-packages (from botocore<1.42.0,>=1.41.3->boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/codespace/.local/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.42.0,>=1.41.3->boto3>=1.40.14->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (1.17.0)\n",
      "Collecting fastavro<2.0.0,>=1.9.4 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.8 kB)\n",
      "Collecting httpx-sse==0.4.0 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
      "Collecting tokenizers<1,>=0.15 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting types-requests<3.0.0,>=2.0.0 (from cohere>=5.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading types_requests-2.32.4.20250913-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/codespace/.local/lib/python3.12/site-packages (from requests>=2.32.2->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (3.4.4)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (3.19.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2025.9.0)\n",
      "Collecting hf-xet<2.0.0,>=1.2.0 (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: packaging>=20.9 in /home/codespace/.local/lib/python3.12/site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (25.0)\n",
      "Collecting shellingham (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/python/3.12.1/lib/python3.12/site-packages (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (4.67.1)\n",
      "Collecting typer-slim (from huggingface-hub>=0.33.5->huggingface-hub[inference]>=0.33.5; extra == \"huggingface\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading typer_slim-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting authlib>=1.6.5 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading authlib-1.6.5-py2.py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting cyclopts>=4.0.0 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading cyclopts-4.3.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting exceptiongroup>=1.2.2 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading exceptiongroup-1.3.1-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting jsonschema-path>=0.3.4 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading jsonschema_path-0.3.4-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting openapi-pydantic>=0.5.1 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading openapi_pydantic-0.5.1-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: platformdirs>=4.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (4.5.0)\n",
      "Collecting py-key-value-aio<0.3.0,>=0.2.8 (from py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading py_key_value_aio-0.2.8-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting python-dotenv>=1.1.0 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting uvicorn>=0.35 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting websockets>=15.0.1 (from fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /home/codespace/.local/lib/python3.12/site-packages (from mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (4.25.1)\n",
      "Collecting pydantic-settings>=2.5.2 (from mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading pydantic_settings-2.12.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting pyjwt>=2.10.1 (from pyjwt[crypto]>=2.10.1->mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading PyJWT-2.10.1-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting python-multipart>=0.0.9 (from mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting sse-starlette>=1.6.1 (from mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading sse_starlette-3.0.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting beartype>=0.22.2 (from py-key-value-aio<0.3.0,>=0.2.8->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading beartype-0.22.6-py3-none-any.whl.metadata (36 kB)\n",
      "Collecting py-key-value-shared==0.2.8 (from py-key-value-aio<0.3.0,>=0.2.8->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading py_key_value_shared-0.2.8-py3-none-any.whl.metadata (682 bytes)\n",
      "Collecting diskcache>=5.6.0 (from py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting pathvalidate>=3.3.1 (from py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading pathvalidate-3.3.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting keyring>=25.6.0 (from py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading keyring-25.7.0-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting cachetools>=6.0.0 (from py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading cachetools-6.2.2-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting cryptography (from authlib>=1.6.5->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Using cached cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: attrs>=23.1.0 in /home/codespace/.local/lib/python3.12/site-packages (from cyclopts>=4.0.0->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (25.4.0)\n",
      "Collecting rich-rst<2.0.0,>=1.3.1 (from cyclopts>=4.0.0->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading rich_rst-1.3.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting docutils (from rich-rst<2.0.0,>=1.3.1->cyclopts>=4.0.0->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading docutils-0.22.3-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading pyasn1_modules-0.4.2-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading rsa-4.9.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting pyasn1>=0.1.3 (from rsa<5,>=3.1.4->google-auth>=2.36.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: colorama>=0.4 in /home/codespace/.local/lib/python3.12/site-packages (from griffe>=1.3.2->pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.4.6)\n",
      "\u001b[33mWARNING: huggingface-hub 1.1.5 does not provide the extra 'inference'\u001b[0m\u001b[33m\n",
      "\u001b[0mRequirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2025.9.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.37.0)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.28.0)\n",
      "Collecting pathable<0.5.0,>=0.4.1 (from jsonschema-path>=0.3.4->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading pathable-0.4.4-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=4.20.0->mcp>=1.18.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading referencing-0.36.2-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting SecretStorage>=3.2 (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading secretstorage-3.5.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting jeepney>=0.4.2 (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading jeepney-0.9.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting jaraco.classes (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading jaraco.classes-3.4.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting jaraco.functools (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading jaraco_functools-4.3.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting jaraco.context (from keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading jaraco.context-6.0.1-py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: executing>=2.0.1 in /home/codespace/.local/lib/python3.12/site-packages (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.2.1)\n",
      "Collecting opentelemetry-exporter-otlp-proto-http<1.39.0,>=1.35.0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting opentelemetry-instrumentation>=0.41b0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting opentelemetry-sdk<1.39.0,>=1.35.0 (from logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting googleapis-common-protos~=1.52 (from opentelemetry-exporter-otlp-proto-http<1.39.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading googleapis_common_protos-1.72.0-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting opentelemetry-exporter-otlp-proto-common==1.38.0 (from opentelemetry-exporter-otlp-proto-http<1.39.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting opentelemetry-proto==1.38.0 (from opentelemetry-exporter-otlp-proto-http<1.39.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading opentelemetry_proto-1.38.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting importlib-metadata<8.8.0,>=6.0 (from opentelemetry-api>=1.28.0->pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading importlib_metadata-8.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
      "Collecting zipp>=3.20 (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.28.0->pydantic-ai-slim==1.22.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading zipp-3.23.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Collecting opentelemetry-semantic-conventions==0.59b0 (from opentelemetry-sdk<1.39.0,>=1.35.0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting opentelemetry-instrumentation-httpx>=0.42b0 (from logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading opentelemetry_instrumentation_httpx-0.59b0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Collecting eval-type-backport>=0.2.0 (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading eval_type_backport-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting invoke<3.0.0,>=2.2.0 (from mistralai>=1.9.10->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading invoke-2.2.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting wrapt<2.0.0,>=1.0.0 (from opentelemetry-instrumentation>=0.41b0->logfire>=3.14.1->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting opentelemetry-util-http==0.59b0 (from opentelemetry-instrumentation-httpx>=0.42b0->logfire[httpx]>=3.14.1; extra == \"logfire\"->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading opentelemetry_util_http-0.59b0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: wcwidth in /home/codespace/.local/lib/python3.12/site-packages (from prompt-toolkit>=3->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (0.2.14)\n",
      "Collecting email-validator>=2.0.0 (from pydantic[email]>=2.11.7->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading email_validator-2.3.0-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting dnspython>=2.0.0 (from email-validator>=2.0.0->pydantic[email]>=2.11.7->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading dnspython-2.8.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: cffi>=2.0.0 in /home/codespace/.local/lib/python3.12/site-packages (from cryptography->authlib>=1.6.5->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.0.0)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.12/site-packages (from cffi>=2.0.0->cryptography->authlib>=1.6.5->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.23)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading markdown_it_py-4.0.0-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /home/codespace/.local/lib/python3.12/site-packages (from rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=13->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting click>=7.0 (from uvicorn>=0.35->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting more-itertools (from jaraco.classes->keyring>=25.6.0->py-key-value-aio[disk,keyring,memory]<0.3.0,>=0.2.8->fastmcp>=2.12.0->pydantic-ai-slim[ag-ui,anthropic,bedrock,cli,cohere,evals,fastmcp,google,groq,huggingface,logfire,mcp,mistral,openai,retries,temporal,ui,vertexai]==1.22.0->pydantic-ai)\n",
      "  Downloading more_itertools-10.8.0-py3-none-any.whl.metadata (39 kB)\n",
      "Downloading pydantic_ai-1.22.0-py3-none-any.whl (11 kB)\n",
      "Downloading pydantic_ai_slim-1.22.0-py3-none-any.whl (414 kB)\n",
      "Downloading pydantic_evals-1.22.0-py3-none-any.whl (56 kB)\n",
      "Downloading pydantic_graph-1.22.0-py3-none-any.whl (72 kB)\n",
      "Downloading temporalio-1.19.0-cp310-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m83.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nexus_rpc-1.1.0-py3-none-any.whl (27 kB)\n",
      "Downloading protobuf-6.33.1-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
      "Downloading ag_ui_protocol-0.1.10-py3-none-any.whl (7.9 kB)\n",
      "Downloading anthropic-0.75.0-py3-none-any.whl (388 kB)\n",
      "Downloading docstring_parser-0.17.0-py3-none-any.whl (36 kB)\n",
      "Downloading argcomplete-3.6.3-py3-none-any.whl (43 kB)\n",
      "Downloading boto3-1.41.3-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.41.3-py3-none-any.whl (14.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.3/14.3 MB\u001b[0m \u001b[31m87.5 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.15.0-py3-none-any.whl (85 kB)\n",
      "Downloading cohere-5.20.0-py3-none-any.whl (303 kB)\n",
      "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
      "Downloading fastavro-1.12.1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (3.5 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.5/3.5 MB\u001b[0m \u001b[31m75.1 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.22.1-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-1.1.5-py3-none-any.whl (516 kB)\n",
      "Downloading hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading types_requests-2.32.4.20250913-py3-none-any.whl (20 kB)\n",
      "Downloading fastmcp-2.13.1-py3-none-any.whl (376 kB)\n",
      "Downloading mcp-1.22.0-py3-none-any.whl (175 kB)\n",
      "Downloading py_key_value_aio-0.2.8-py3-none-any.whl (69 kB)\n",
      "Downloading py_key_value_shared-0.2.8-py3-none-any.whl (14 kB)\n",
      "Downloading authlib-1.6.5-py2.py3-none-any.whl (243 kB)\n",
      "Downloading beartype-0.22.6-py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-6.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading cyclopts-4.3.0-py3-none-any.whl (187 kB)\n",
      "Downloading rich_rst-1.3.2-py3-none-any.whl (12 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "Downloading exceptiongroup-1.3.1-py3-none-any.whl (16 kB)\n",
      "Downloading google_auth-2.43.0-py2.py3-none-any.whl (223 kB)\n",
      "Downloading rsa-4.9.1-py3-none-any.whl (34 kB)\n",
      "Downloading google_genai-1.52.0-py3-none-any.whl (261 kB)\n",
      "Downloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Downloading websockets-15.0.1-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (182 kB)\n",
      "Downloading griffe-1.15.0-py3-none-any.whl (150 kB)\n",
      "Downloading groq-0.36.0-py3-none-any.whl (137 kB)\n",
      "Downloading jsonschema_path-0.3.4-py3-none-any.whl (14 kB)\n",
      "Downloading pathable-0.4.4-py3-none-any.whl (9.6 kB)\n",
      "Downloading referencing-0.36.2-py3-none-any.whl (26 kB)\n",
      "Downloading keyring-25.7.0-py3-none-any.whl (39 kB)\n",
      "Downloading jeepney-0.9.0-py3-none-any.whl (49 kB)\n",
      "Downloading logfire-4.15.1-py3-none-any.whl (228 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_http-1.38.0-py3-none-any.whl (19 kB)\n",
      "Downloading opentelemetry_exporter_otlp_proto_common-1.38.0-py3-none-any.whl (18 kB)\n",
      "Downloading opentelemetry_proto-1.38.0-py3-none-any.whl (72 kB)\n",
      "Downloading googleapis_common_protos-1.72.0-py3-none-any.whl (297 kB)\n",
      "Downloading opentelemetry_api-1.38.0-py3-none-any.whl (65 kB)\n",
      "Downloading importlib_metadata-8.7.0-py3-none-any.whl (27 kB)\n",
      "Downloading opentelemetry_sdk-1.38.0-py3-none-any.whl (132 kB)\n",
      "Downloading opentelemetry_semantic_conventions-0.59b0-py3-none-any.whl (207 kB)\n",
      "Downloading logfire_api-4.15.1-py3-none-any.whl (95 kB)\n",
      "Downloading mistralai-1.9.11-py3-none-any.whl (442 kB)\n",
      "Downloading invoke-2.2.1-py3-none-any.whl (160 kB)\n",
      "Downloading eval_type_backport-0.3.0-py3-none-any.whl (6.1 kB)\n",
      "Downloading openapi_pydantic-0.5.1-py3-none-any.whl (96 kB)\n",
      "Downloading opentelemetry_instrumentation-0.59b0-py3-none-any.whl (33 kB)\n",
      "Downloading wrapt-1.17.3-cp312-cp312-manylinux1_x86_64.manylinux_2_28_x86_64.manylinux_2_5_x86_64.whl (88 kB)\n",
      "Downloading opentelemetry_instrumentation_httpx-0.59b0-py3-none-any.whl (15 kB)\n",
      "Downloading opentelemetry_util_http-0.59b0-py3-none-any.whl (7.6 kB)\n",
      "Downloading pathvalidate-3.3.1-py3-none-any.whl (24 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyasn1_modules-0.4.2-py3-none-any.whl (181 kB)\n",
      "Downloading pydantic_settings-2.12.0-py3-none-any.whl (51 kB)\n",
      "Downloading email_validator-2.3.0-py3-none-any.whl (35 kB)\n",
      "Downloading dnspython-2.8.0-py3-none-any.whl (331 kB)\n",
      "Downloading PyJWT-2.10.1-py3-none-any.whl (22 kB)\n",
      "Using cached cryptography-46.0.3-cp311-abi3-manylinux_2_34_x86_64.whl (4.5 MB)\n",
      "Downloading pyperclip-1.11.0-py3-none-any.whl (11 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading rich-14.2.0-py3-none-any.whl (243 kB)\n",
      "Downloading markdown_it_py-4.0.0-py3-none-any.whl (87 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading secretstorage-3.5.0-py3-none-any.whl (15 kB)\n",
      "Downloading sse_starlette-3.0.3-py3-none-any.whl (11 kB)\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Downloading types_protobuf-6.32.1.20251105-py3-none-any.whl (77 kB)\n",
      "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
      "Downloading zipp-3.23.0-py3-none-any.whl (10 kB)\n",
      "Downloading docutils-0.22.3-py3-none-any.whl (633 kB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m633.0/633.0 kB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jaraco.classes-3.4.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco.context-6.0.1-py3-none-any.whl (6.8 kB)\n",
      "Downloading jaraco_functools-4.3.0-py3-none-any.whl (10 kB)\n",
      "Downloading more_itertools-10.8.0-py3-none-any.whl (69 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typer_slim-0.20.0-py3-none-any.whl (47 kB)\n",
      "Installing collected packages: pyperclip, zipp, wrapt, websockets, types-requests, types-protobuf, tenacity, shellingham, referencing, python-multipart, python-dotenv, pyjwt, pyasn1, protobuf, pathvalidate, pathable, opentelemetry-util-http, nexus-rpc, more-itertools, mdurl, logfire-api, jmespath, jeepney, jaraco.context, invoke, httpx-sse, hf-xet, griffe, fastavro, exceptiongroup, eval-type-backport, docutils, docstring-parser, dnspython, diskcache, click, cachetools, beartype, argcomplete, uvicorn, typer-slim, temporalio, starlette, sse-starlette, rsa, pyasn1-modules, py-key-value-shared, opentelemetry-proto, markdown-it-py, jsonschema-path, jaraco.functools, jaraco.classes, importlib-metadata, googleapis-common-protos, email-validator, cryptography, botocore, SecretStorage, s3transfer, rich, pydantic-settings, pydantic-graph, py-key-value-aio, opentelemetry-exporter-otlp-proto-common, opentelemetry-api, openapi-pydantic, mistralai, huggingface-hub, groq, google-auth, authlib, anthropic, ag-ui-protocol, tokenizers, rich-rst, pydantic-ai-slim, opentelemetry-semantic-conventions, mcp, keyring, google-genai, boto3, pydantic-evals, opentelemetry-sdk, opentelemetry-instrumentation, cyclopts, cohere, opentelemetry-instrumentation-httpx, opentelemetry-exporter-otlp-proto-http, fastmcp, logfire, pydantic-ai\n",
      "\u001b[2K  Attempting uninstall: referencing49;38;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/91\u001b[0m [tenacity]\n",
      "\u001b[2K    Found existing installation: referencing 0.37.05;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/91\u001b[0m [tenacity]\n",
      "\u001b[2K    Uninstalling referencing-0.37.0:;114m╸\u001b[0m\u001b[38;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/91\u001b[0m [tenacity]\n",
      "\u001b[2K      Successfully uninstalled referencing-0.37.08;5;237m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m 6/91\u001b[0m [tenacity]\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91/91\u001b[0m [pydantic-ai]m [logfire]32m88/91\u001b[0m [fastmcp]]k]i]lim]\n",
      "\u001b[1A\u001b[2KSuccessfully installed SecretStorage-3.5.0 ag-ui-protocol-0.1.10 anthropic-0.75.0 argcomplete-3.6.3 authlib-1.6.5 beartype-0.22.6 boto3-1.41.3 botocore-1.41.3 cachetools-6.2.2 click-8.3.1 cohere-5.20.0 cryptography-46.0.3 cyclopts-4.3.0 diskcache-5.6.3 dnspython-2.8.0 docstring-parser-0.17.0 docutils-0.22.3 email-validator-2.3.0 eval-type-backport-0.3.0 exceptiongroup-1.3.1 fastavro-1.12.1 fastmcp-2.13.1 google-auth-2.43.0 google-genai-1.52.0 googleapis-common-protos-1.72.0 griffe-1.15.0 groq-0.36.0 hf-xet-1.2.0 httpx-sse-0.4.0 huggingface-hub-1.1.5 importlib-metadata-8.7.0 invoke-2.2.1 jaraco.classes-3.4.0 jaraco.context-6.0.1 jaraco.functools-4.3.0 jeepney-0.9.0 jmespath-1.0.1 jsonschema-path-0.3.4 keyring-25.7.0 logfire-4.15.1 logfire-api-4.15.1 markdown-it-py-4.0.0 mcp-1.22.0 mdurl-0.1.2 mistralai-1.9.11 more-itertools-10.8.0 nexus-rpc-1.1.0 openapi-pydantic-0.5.1 opentelemetry-api-1.38.0 opentelemetry-exporter-otlp-proto-common-1.38.0 opentelemetry-exporter-otlp-proto-http-1.38.0 opentelemetry-instrumentation-0.59b0 opentelemetry-instrumentation-httpx-0.59b0 opentelemetry-proto-1.38.0 opentelemetry-sdk-1.38.0 opentelemetry-semantic-conventions-0.59b0 opentelemetry-util-http-0.59b0 pathable-0.4.4 pathvalidate-3.3.1 protobuf-6.33.1 py-key-value-aio-0.2.8 py-key-value-shared-0.2.8 pyasn1-0.6.1 pyasn1-modules-0.4.2 pydantic-ai-1.22.0 pydantic-ai-slim-1.22.0 pydantic-evals-1.22.0 pydantic-graph-1.22.0 pydantic-settings-2.12.0 pyjwt-2.10.1 pyperclip-1.11.0 python-dotenv-1.2.1 python-multipart-0.0.20 referencing-0.36.2 rich-14.2.0 rich-rst-1.3.2 rsa-4.9.1 s3transfer-0.15.0 shellingham-1.5.4 sse-starlette-3.0.3 starlette-0.50.0 temporalio-1.19.0 tenacity-9.1.2 tokenizers-0.22.1 typer-slim-0.20.0 types-protobuf-6.32.1.20251105 types-requests-2.32.4.20250913 uvicorn-0.38.0 websockets-15.0.1 wrapt-1.17.3 zipp-3.23.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pydantic-ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "08e0bdf9-248c-490f-9ea8-ebcc7e2d5c77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'name': 'search',\n",
       " 'description': 'Search the FAQ database',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'query': {'type': 'string',\n",
       "    'description': 'Search query text to look up in the course FAQ.'}},\n",
       "  'required': ['query'],\n",
       "  'additionalProperties': False}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ffaa2c1-2ee3-4e6b-aa7b-47c8160cfbd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent\n",
    "search_agent = Agent(\n",
    "    model='openai:gpt-4o-mini',\n",
    "    instructions=developer_prompt,\n",
    "    tools=[search],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d50e8e82-9211-4f42-ae07-057a0bac751b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await search_agent.run(user_prompt=question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4731283b-9825-4abc-b2bd-2e64a70577e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the Data Engineering Zoomcamp course, even though it may have already started. You are able to start learning and submitting homework without needing to officially register, as the submissions aren't checked against a registered list. However, it's important to keep in mind that there are deadlines for turning in final projects, so you should try to stay on top of those.\n",
      "\n",
      "To get the most out of the course, I recommend registering through the provided links as soon as possible. This will allow you to access materials and stay updated with any announcements related to the course.\n",
      "\n",
      "If you have any specific concerns or areas related to joining the course you'd like to delve deeper into, please let me know! Is there anything specific regarding materials, deadlines, or getting started that you want to discuss further?\n"
     ]
    }
   ],
   "source": [
    "print(result.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "97c906cd-5994-4a18-b36b-6740c058766c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunUsage(input_tokens=2850, output_tokens=199, details={'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, requests=3, tool_calls=2)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.usage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "cab5ac4a-72bb-4021-b455-e2e3da752593",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'calc_price' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mcalc_price\u001b[49m(result.udate(),\u001b[33m'\u001b[39m\u001b[33mgpt-4o-mini\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mopenai\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'calc_price' is not defined"
     ]
    }
   ],
   "source": [
    "calc_price(result.udate(),'gpt-4o-mini','openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "2c1c2622-2bbf-4c56-8ebe-b7aad8d77de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.chat.runners import PydanticAIRunner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bfbeaac-7774-4226-810c-feb517edb583",
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = PydanticAIRunner(\n",
    "        chat_interface=chat_interface,\n",
    "        agent=search_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7ee0e169-adce-4682-bfbe-087084cef4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: hOw tO install kafka?\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'replace'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[56]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m runner.run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/toyaikit/chat/runners.py:376\u001b[39m, in \u001b[36mPydanticAIRunner.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    374\u001b[39m             call = tool_calls[call_id]\n\u001b[32m    375\u001b[39m             result = part.content\n\u001b[32m--> \u001b[39m\u001b[32m376\u001b[39m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchat_interface\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdisplay_function_call\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    377\u001b[39m \u001b[43m                \u001b[49m\u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtool_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdumps\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\n\u001b[32m    378\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m message_history.extend(messages)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/toyaikit/chat/interface.py:69\u001b[39m, in \u001b[36mIPythonChatInterface.display_function_call\u001b[39m\u001b[34m(self, function_name, arguments, result)\u001b[39m\n\u001b[32m     66\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdisplay_function_call\u001b[39m(\n\u001b[32m     67\u001b[39m     \u001b[38;5;28mself\u001b[39m, function_name: \u001b[38;5;28mstr\u001b[39m, arguments: \u001b[38;5;28mstr\u001b[39m, result: \u001b[38;5;28mstr\u001b[39m\n\u001b[32m     68\u001b[39m ) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     excaped_result = \u001b[43mresult\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreplace\u001b[49m(\u001b[33m\"\u001b[39m\u001b[33m<\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m&lt;\u001b[39m\u001b[33m\"\u001b[39m).replace(\u001b[33m\"\u001b[39m\u001b[33m>\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m&gt;\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     70\u001b[39m     call_html = \u001b[33mf\u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[33m        <details>\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[33m        <summary>Function call: <tt>\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunction_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshorten(arguments)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)</tt></summary>\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     82\u001b[39m \u001b[33m        </details>\u001b[39m\n\u001b[32m     83\u001b[39m \u001b[33m    \u001b[39m\u001b[33m\"\"\"\u001b[39m\n\u001b[32m     84\u001b[39m     ip_display(HTML(call_html))\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'replace'"
     ]
    }
   ],
   "source": [
    "await runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c5aa6ac4-713e-4986-a687-474c2369acbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(question: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Add a new entry to the FAQ database.\n",
    "\n",
    "    Args:\n",
    "        question (str): The question to be added to the FAQ database.\n",
    "        answer (str): The corresponding answer to the question.\n",
    "    \"\"\"\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "267d155a-b4b7-4a8b-82dc-ab99dfa47924",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_tools.add_tool(add_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d703eba1-f9e5-4c11-af97-84e3b250eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "50a8d67b-2a27-4e98-b6a7-f3b248c00f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: HOW DO I DO WELL IN MODULE 1?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"do well in module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"do well in module 1\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
       "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex&lt;&lt;t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\\\"Murray Hill\\\") & (new_final.b_zone==\\\"Midwood\\\")).show()\\nKrishna Anand\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Error raised during the jupyter notebook\\u2019s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module \\u201cpsycopg2\\u201d. Can be installed by Conda or pip.\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"tips for success in module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"tips for success in module 1\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex&lt;&lt;t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\\\"Murray Hill\\\") & (new_final.b_zone==\\\"Midwood\\\")).show()\\nKrishna Anand\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
       "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \\\"TypeError: 'module' object is not callable\\\"\\nSolution:\\nconn_string = \\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\"\\nengine = create_engine(conn_string)\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To excel in Module 1 of your course, here are some effective strategies and key considerations:</p>\n",
       "<ol>\n",
       "<li><p><strong>Understand the Content</strong>: Module 1 focuses on Docker and Terraform, essential tools for data engineering. Familiarize yourself with their basic concepts and functionalities.</p>\n",
       "</li>\n",
       "<li><p><strong>Hands-On Practice</strong>: Set up your local environment as per the course instructions. Installing and configuring Docker and Terraform on your machine will help reinforce your understanding.</p>\n",
       "</li>\n",
       "<li><p><strong>Common Issues</strong>: Be aware of common errors, such as <code>ModuleNotFoundError</code> related to packages like 'psycopg2'. Learning how to diagnose and resolve these issues will aid your progress. For instance, ensure that you install necessary packages with:</p>\n",
       "<pre><code class=\"language-bash\">pip install psycopg2-binary\n",
       "</code></pre>\n",
       "</li>\n",
       "<li><p><strong>Follow Instructions Carefully</strong>: When working with Docker, follow the instructions step by step. For example, after running <code>docker-compose build</code>, make sure no errors appear, and check for missing packages.</p>\n",
       "</li>\n",
       "<li><p><strong>Engage with Resources</strong>: Utilize any available forums or study groups. Asking questions and discussing with peers can provide deeper insights and clarify doubts.</p>\n",
       "</li>\n",
       "<li><p><strong>Ask for Help</strong>: If you encounter problems, like with database connections, seek help from course resources or community forums. For example, if you face <code>ModuleNotFoundError</code>, check that you are using the correct package.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Do you find these strategies helpful? Are there specific areas within Module 1 that you want to explore further?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: DOCKER\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"docker module 1 tips\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"query\":\"docker module 1 tips\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
       "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Error raised during the jupyter notebook\\u2019s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module \\u201cpsycopg2\\u201d. Can be installed by Conda or pip.\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \\\"TypeError: 'module' object is not callable\\\"\\nSolution:\\nconn_string = \\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\"\\nengine = create_engine(conn_string)\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "    \"course\": \"data-engineering-zoomcamp\"\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Here are some tips to help you succeed with Docker in Module 1:</p>\n",
       "<ol>\n",
       "<li><p><strong>Set Up Your Environment</strong>: Ensure Docker is properly installed and configured on your machine. Follow the course instructions precisely to set up the Docker environment.</p>\n",
       "</li>\n",
       "<li><p><strong>Familiarize with Docker Commands</strong>:</p>\n",
       "<ul>\n",
       "<li>Learn about essential commands like <code>docker build</code>, <code>docker run</code>, and <code>docker-compose</code>. Understanding how to use these commands is crucial for managing your Docker containers effectively.</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Resolve Common Errors</strong>:</p>\n",
       "<ul>\n",
       "<li>Be prepared to troubleshoot common issues. For example, if you see an error like <code>ModuleNotFoundError: No module named 'psycopg2'</code>, ensure that you install the required package using:<pre><code class=\"language-bash\">pip install psycopg2-binary\n",
       "</code></pre>\n",
       "</li>\n",
       "</ul>\n",
       "</li>\n",
       "<li><p><strong>Use Dockerfile and docker-compose.yml</strong>: Understand how to write and modify your Dockerfile and docker-compose.yaml files. These files define the environment and dependencies necessary for your application.</p>\n",
       "</li>\n",
       "<li><p><strong>Check Logs for Debugging</strong>: If you encounter errors when running your containers, inspect the logs using:</p>\n",
       "<pre><code class=\"language-bash\">docker-compose logs\n",
       "</code></pre>\n",
       "<p>This will provide insights into what might be going wrong.</p>\n",
       "</li>\n",
       "<li><p><strong>Practice Containers</strong>: Create simple Docker containers for practice. Experiment with building images and running them to get comfortable with the workflow.</p>\n",
       "</li>\n",
       "<li><p><strong>Engage with Resources</strong>: Utilize any provided documentation and forums to ask questions and learn from others’ experiences. Community support can be invaluable.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Are there any specific Docker-related topics or issues you want to dive deeper into?</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: add this back tO FAQ\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>add_entry({\"question\":\"How do I do well with Docker in Mo...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>{\"question\":\"How do I do well with Docker in Module 1?\",\"answer\":\"1. **Set Up Your Environment**: Ensure Docker is properly installed and configured on your machine. Follow the course instructions precisely to set up the Docker environment.\\n\\n2. **Familiarize with Docker Commands**: Learn about essential commands like `docker build`, `docker run`, and `docker-compose`. Understanding how to use these commands is crucial for managing your Docker containers effectively.\\n\\n3. **Resolve Common Errors**: Be prepared to troubleshoot common issues. For example, if you see an error like `ModuleNotFoundError: No module named 'psycopg2'`, ensure that you install the required package using:\\n   ```bash\\n   pip install psycopg2-binary\\n   ```\\n\\n4. **Use Dockerfile and docker-compose.yml**: Understand how to write and modify your Dockerfile and docker-compose.yaml files. These files define the environment and dependencies necessary for your application.\\n\\n5. **Check Logs for Debugging**: If you encounter errors when running your containers, inspect the logs using:\\n   ```bash\\n   docker-compose logs\\n   ```\\n   This will provide insights into what might be going wrong.\\n\\n6. **Practice Containers**: Create simple Docker containers for practice. Experiment with building images and running them to get comfortable with the workflow.\\n\\n7. **Engage with Resources**: Utilize any provided documentation and forums to ask questions and learn from others’ experiences. Community support can be invaluable.\"}</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>null</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Response received\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>I've added the information about doing well with Docker in Module 1 to the FAQ. If you have any other questions or need further assistance, feel free to ask!</p>\n",
       "</div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: STOP\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "result = runner.run();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8d17c9db-6885-44ed-a929-bef6135bfa4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'How do I do well with Docker in Module 1?',\n",
       " 'text': \"1. **Set Up Your Environment**: Ensure Docker is properly installed and configured on your machine. Follow the course instructions precisely to set up the Docker environment.\\n\\n2. **Familiarize with Docker Commands**: Learn about essential commands like `docker build`, `docker run`, and `docker-compose`. Understanding how to use these commands is crucial for managing your Docker containers effectively.\\n\\n3. **Resolve Common Errors**: Be prepared to troubleshoot common issues. For example, if you see an error like `ModuleNotFoundError: No module named 'psycopg2'`, ensure that you install the required package using:\\n   ```bash\\n   pip install psycopg2-binary\\n   ```\\n\\n4. **Use Dockerfile and docker-compose.yml**: Understand how to write and modify your Dockerfile and docker-compose.yaml files. These files define the environment and dependencies necessary for your application.\\n\\n5. **Check Logs for Debugging**: If you encounter errors when running your containers, inspect the logs using:\\n   ```bash\\n   docker-compose logs\\n   ```\\n   This will provide insights into what might be going wrong.\\n\\n6. **Practice Containers**: Create simple Docker containers for practice. Experiment with building images and running them to get comfortable with the workflow.\\n\\n7. **Engage with Resources**: Utilize any provided documentation and forums to ask questions and learn from others’ experiences. Community support can be invaluable.\",\n",
       " 'section': 'user added',\n",
       " 'course': 'data-engineering-zoomcamp'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.docs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "1ab73436-677e-4bd3-83e4-336ddfde83ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'Problem description\\nProject structure:\\n/sources/production/model_service.py\\n/sources/tests/unit_tests/test_model_service.py (“from production.model_service import ModelService)\\ngit commit -t ‘test’ raises ‘No module named ‘production’’ when calling pytest hook\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: pytest\\nlanguage: system\\npass_filenames: false\\nalways_run: true\\nargs: [\\n\"tests/\"\\n]\\nSolution description\\nUse this hook instead:\\n- repo: local\\nhooks:\\n- id: pytest-check\\nname: pytest-check\\nentry: \"./sources/tests/unit_tests/run.sh\"\\nlanguage: system\\ntypes: [python]\\npass_filenames: false\\nalways_run: true\\nAnd make sure that run.sh sets the right directory and run pytest:\\ncd \"$(dirname \"$0\")\"\\ncd ../..\\nexport PYTHONPATH=.\\npipenv run pytest ./tests/unit_tests\\nAdded by MarcosMJD',\n",
       "  'section': 'Module 6: Best practices',\n",
       "  'question': 'Pytest error ‘module not found’ when using pre-commit hooks if using custom packages in the source code',\n",
       "  'course': 'mlops-zoomcamp'},\n",
       " {'text': 'First check if SSL module configured with following command:\\nPython -m ssl\\n\\nIf the output of this is empty there is no problem with SSL configuration.\\n\\nThen you should upgrade your pipenv package in your current environment to resolve the problem.\\nAdded by Kenan Arslanbay',\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"module is not available (Can't connect to HTTPS URL)\",\n",
       "  'course': 'mlops-zoomcamp'},\n",
       " {'text': \"If one gets pipenv failures for pipenv install command -\\nAttributeError: module 'collections' has no attribute 'MutableMapping'\\nIt happens because you use the system Python (3.10) for pipenv.\\nIf you previously installed pipenv with apt-get, remove it - sudo-apt remove pipenv\\nMake sure you have a non-system Python installed in your environment. The easiest way to do it is to install anaconda or miniconda\\nNext, install pipenv to your non-system Python. If you use the setup from the lectures, it’s just this: pip install pipenv\\nNow re-run pipenv install XXXX (relevant dependencies) - should work\\nTested and worked on AWS instance, similar to the config Alexey presented in class.\\nAdded by Daniel HenSSL\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"Pipenv installation not working (AttributeError: module 'collections' has no attribute 'MutableMapping')\",\n",
       "  'course': 'mlops-zoomcamp'},\n",
       " {'text': \"During scikit-learn installation via the command:\\npipenv install scikit-learn==1.0.2\\nThe following error is raised:\\nModuleNotFoundError: No module named 'pip._vendor.six'\\nThen, one should:\\nsudo apt install python-six\\npipenv --rm\\npipenv install scikit-learn==1.0.2\\nAdded by Giovanni Pecoraro\",\n",
       "  'section': 'Module 4: Deployment',\n",
       "  'question': \"No module named 'pip._vendor.six'\",\n",
       "  'course': 'mlops-zoomcamp'},\n",
       " {'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Module Not Found Error in Jupyter Notebook .',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"Issue:\\ne…\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': \"Error raised during the jupyter notebook’s cell execution:\\nengine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\\nSolution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\",\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'i\\'m getting this error ModuleNotFoundError: No module named \\'pipenv.patched.pip._vendor.urllib3.response\\'\\nand Resolved from this command pip install pipenv --force-reinstall\\ngetting this errror site-packages\\\\pipenv\\\\patched\\\\pip\\\\_vendor\\\\urllib3\\\\connectionpool.py\"\\nResolved from this command pip install -U pip and pip install requests\\nAsif',\n",
       "  'section': 'Module 5: Monitoring',\n",
       "  'question': \"ModuleNotFoundError: No module named 'pipenv.patched.pip._vendor.urllib3.response'\",\n",
       "  'course': 'mlops-zoomcamp'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search(\"How do i do in Module 1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "112d6d2b-304b-4343-970c-e57e63bf62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any\n",
    "class SearchTools:\n",
    "\n",
    "    def __init__(self, index):\n",
    "        self.index = index\n",
    "\n",
    "    def search(self, query: str) -> List[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Search the FAQ database for entries matching the given query.\n",
    "    \n",
    "        Args:\n",
    "            query (str): Search query text to look up in the course FAQ.\n",
    "    \n",
    "        Returns:\n",
    "            List[Dict[str, Any]]: A list of search result entries, each containing relevant metadata.\n",
    "        \"\"\"\n",
    "        boost = {'question': 3.0, 'section': 0.5}\n",
    "    \n",
    "        results = self.index.search(\n",
    "            query=query,\n",
    "            filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "            boost_dict=boost,\n",
    "            num_results=5,\n",
    "            output_ids=True\n",
    "        )\n",
    "    \n",
    "        return results\n",
    "\n",
    "    def add_entry(self, question: str, answer: str) -> None:\n",
    "        \"\"\"\n",
    "        Add a new entry to the FAQ database.\n",
    "    \n",
    "        Args:\n",
    "            question (str): The question to be added to the FAQ database.\n",
    "            answer (str): The corresponding answer to the question.\n",
    "        \"\"\"\n",
    "        doc = {\n",
    "            'question': question,\n",
    "            'text': answer,\n",
    "            'section': 'user added',\n",
    "            'course': 'data-engineering-zoomcamp'\n",
    "        }\n",
    "        self.index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "28172c93-17a5-4982-a4d8-5c969d810a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tools = SearchTools(index)\n",
    "\n",
    "agent_tools = Tools()\n",
    "agent_tools.add_tools(search_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "582f6e28-c552-431e-82dd-4b1ceadaf271",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toyaikit.tools import get_instance_methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3773ebed-bf83-4ba9-a824-880cfcd4df9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<bound method SearchTools.add_entry of <__main__.SearchTools object at 0x7d6c19ae9a30>>,\n",
       " <bound method SearchTools.search of <__main__.SearchTools object at 0x7d6c19ae9a30>>]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_instance_methods(search_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3b72dc-9500-4c71-a8d6-710297afd6ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'IPythonChatInterface' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m chat_interface = \u001b[43mIPythonChatInterface\u001b[49m()\n\u001b[32m      3\u001b[39m runner = OpenAIResponsesRunner(\n\u001b[32m      4\u001b[39m     tools=agent_tools,\n\u001b[32m      5\u001b[39m     developer_prompt=developer_prompt,\n\u001b[32m      6\u001b[39m     chat_interface=chat_interface,\n\u001b[32m      7\u001b[39m     llm_client=OpenAIClient()\n\u001b[32m      8\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'IPythonChatInterface' is not defined"
     ]
    }
   ],
   "source": [
    "chat_interface = IPythonChatInterface()\n",
    "\n",
    "runner = OpenAIResponsesRunner(\n",
    "    tools=agent_tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    llm_client=OpenAIClient()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4b0b3e6-b61f-4983-8fea-ea600e4e1767",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'runner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mrunner\u001b[49m.run()\n",
      "\u001b[31mNameError\u001b[39m: name 'runner' is not defined"
     ]
    }
   ],
   "source": [
    "runner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d5113-0d8d-4a47-abb7-c2917ef77fa8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
